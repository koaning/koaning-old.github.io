	<meta name='viewport' content="width=device-width">
    <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/d3/3.4.1/d3.min.js"></script>
	<script src="//ajax.googleapis.com/ajax/libs/jqueryui/1.10.4/jquery-ui.min.js"></script>
    <script src="http://netdna.bootstrapcdn.com/bootstrap/3.1.1/js/bootstrap.min.js"></script>
	<script src="/js/prism.js"></script>

    <link rel="stylesheet" href="http://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css">
    <link rel="stylesheet" href="http://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap-theme.min.css">
    <link rel="stylesheet" href="//ajax.googleapis.com/ajax/libs/jqueryui/1.10.4/themes/smoothness/jquery-ui.css" />
	<link rel="stylesheet" href="/css/prism.css">
	<link rel="stylesheet" href="../css/style.css"></link>
	<style>
	body {
	  padding-top: 30px;
	}
	.starter-template {
	  padding: 40px 15px;
	  text-align: center;
	}
	pre{
		border:0px;
		border-radius: 0px; 
	}
	code{
		background-color: #f5f5f5;
		color: black;
	}

	iframe.graph{
		border: 0;
	    display: block;
	    margin-left: auto;
	    margin-right: auto;
	}
</style>

<div class="container">
	<div class="row">
		<a href="/">back to main</a>	
			<br> <br> 
			
	</div>	
	<h1>Neural PCA/Compression in R</h1>

<p>In this document I will show a small experiment that shows how a neural network can be used as a pca-algorithm. It should also help explain how neural networks work and why neural networks can be such a powerful machine learning algorithm.</p>

<h3>Generating Bits</h3>

<p>I will start out by generation random bits as data in a data frame.</p>

<pre><code>set.seed(1)
num.vars = 6 
num.obs = 500 
df = data.frame(replicate(num.vars,sample(0:1,num.obs,rep=TRUE)))
</code></pre>

<p>This bits will then be used as input <strong>and</strong> output for a neural network. The goal is to see if a neural network can be trained to fit the data back into its original format while only using a limited amount of nodes in the hidden layer. Such a neural network might look like so:</p>

<p><img src="../img/npca1.png" alt="" / class="center-block" height="500"></p>

<p>We can check the performace of such a neural network.</p>

<pre><code>bit.acc = sum( round(mod$fitted.values) == df ) / sum( df == df )
row.acc = sum(rowSums(round(mod$fitted.values) == df) == 6)/nrow(df)
</code></pre>

<p>In this case we have about 91% of the bits estimated correctly and about 58% of the rows estimated correctly. That means that we are able to maintain almost 60% of the variance in this dataset by only using 4 hidden nodes (out of 6 total data sources). This is interesting because right now we are using a neural network to simulate compression and it gives us an alternative to eigenvalue based principle component analysis.</p>

<h3>Repeating for different network sizes</h3>

<p>To further investigate this effect we would need to set up a proper simulation run. Neural network algorithms usually have random initialization values, so I will simulate multiple neural networks per hidden node size. Below I have created the simulation function that I will use throughout the document.</p>

<pre><code>simulation = function(df, num.sims){
  results = data.frame(vars = as.numeric(c()), acc = as.numeric(c()), type=as.factor(c()))
  num.vars = ncol(df)  
  for(i in 1:num.vars){
    for(j in 1:num.sims){
        # cat("hidden nodes :", i ,"sample #", j, "\n")
        mod = mlp(df, df, size=c(i))
        bit.acc = sum( round(mod$fitted.values) == df ) / sum( df == df )
        row.acc = sum(rowSums(round(mod$fitted.values) == df) == num.vars)/nrow(df)
        results = rbind(results, data.frame(vars = i, acc = bit.acc, type= "bit"))
        results = rbind(results, data.frame(vars = i, acc = row.acc, type= "row"))
      }
    }
  results 
}
</code></pre>

<h3>Simulation 1</h3>

<pre><code>num.vars = 8
num.obs = 200 
df = data.frame(replicate(num.vars,sample(0:1,num.obs,rep=TRUE)))
sims = simulation(df, 5)
</code></pre>

<p><img src="../img/npca2.png" alt="" / class="center-block" height="500"></p>

<p>We see the effect that we would expect. It is much easier to predict a correct bit, theres about 50% of guessing a bit correctly so it doesn't come off as suprising that the bit performance is always above 50% accuracy. Estimating the entire row prooves to be much more difficult. We need six out of eight nodes to achieve an accuracy above 50%.</p>

<h3>Simulation 2</h3>

<p>We will now simulate a much larger neural network.</p>

<pre><code>num.vars = 20
num.obs = 200 
df = data.frame(replicate(num.vars,sample(0:1,num.obs,rep=TRUE)))
sims = simulation(df, 5)
</code></pre>

<p><img src="../img/npca3.png" alt="" / class="center-block" height="500"></p>

<p>We see a very similar effect but notice we need more than 10 hidden nodes to achieve 50% accuracy.</p>

<h3>Simulation 3</h3>

<p>We will now simulate the data slightly differently. We will cause some correlation between columns and to show that we need less hidden nodes if the data is correlated. </p>

<pre><code>num.vars = 4
num.obs = 200 
df = data.frame(replicate(num.vars,sample(0:1,num.obs,rep=TRUE)))
df = cbind(df, sapply(df$X1, function(x) if(runif(1)&lt;0.8) x else sample(0:1)[1]))
df = cbind(df, sapply(df$X2, function(x) if(runif(1)&lt;0.8) x else sample(0:1)[1]))
df = cbind(df, sapply(df$X3, function(x) if(runif(1)&lt;0.8) x else sample(0:1)[1]))
df = cbind(df, sapply(df$X4, function(x) if(runif(1)&lt;0.8) x else sample(0:1)[1]))
sims = simulation(df, 5)
</code></pre>

<p><img src="../img/npca4.png" alt="" / class="center-block" height="500"></p>

<h3>Simulation 4</h3>

<p>A similar conclusion can be made when it is more likely for a '1' bit to occur than a '0' bit. </p>

<pre><code>num.vars = 20
num.obs = 200 
df = data.frame(replicate(num.vars,sample(c(0,1,1,1),num.obs,rep=TRUE)))
sims = simulation(df, 5)
</code></pre>

<p><img src="../img/npca5.png" alt="" / class="center-block" height="500"></p>

<h2>Conclusion</h2>

<p>I do somehow feel like only a very niche of humanity cares about this sort of thing. Nonetheless I think I have been able to produce yet another example showing that neural networks are really really cool simply because they you can apply and approach the phenomenon from many different angles.</p>

<p>Feel free to comment/edit. Full Rscript can be found <a href="https://gist.githubusercontent.com/koaning/331610c504fac3be01c8/raw/186e770d5c282a7ad7b04f120ae523fb727abb6d/gistfile1.txt">here</a>.</p>

	<div class="row">
		<div class="col-md-12">
<a href="/">back to main</a>
	</div>
	   		<div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'koaningcom'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


	</div>
</div>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-28011256-5', 'auto');
  ga('send', 'pageview');

</script>